
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chunk Visualization</title>
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAxMDAgMTAwIiB3aWR0aD0iMTAwIiBoZWlnaHQ9IjEwMCI+PHRleHQgeD0iNTAiIHk9IjU1IiBmb250LXNpemU9IjkwIiB0ZXh0LWFuY2hvcj0ibWlkZGxlIiBkb21pbmFudC1iYXNlbGluZT0ibWlkZGxlIj7wn6abPC90ZXh0Pjwvc3ZnPg==">
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol"; line-height: 1.6; padding: 0; margin: 0; background-color: #F0F2F5; color: #333333; display: flex; flex-direction: column; min-height: 100vh; }
        .content-box { max-width: 900px; width: 100%; margin: 30px auto; padding: 30px 20px 20px 20px; background-color: #FFFFFF; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); box-sizing: border-box; }
        .text-display { white-space: pre-wrap; word-wrap: break-word; font-family: "Consolas", "Monaco", "Courier New", monospace; font-size: 0.95em; padding: 0; }
        .text-display span[style*="background-color"] { border-radius: 3px; padding: 0.1em 0; cursor: help; }
        .text-display br { display: block; content: ""; margin-top: 0.6em; }
        footer { text-align: center; margin-top: auto; padding: 15px 0; font-size: 0.8em; color: #888; border-top: 1px solid #eee; background-color: #f0f2f5; width: 100%; }
        footer a { color: #666; text-decoration: none; }
        footer a:hover { text-decoration: underline; }
        footer .heart { color: #d63384; display: inline-block; }
    </style>
</head>
<body>
    
<div class="content-box">
    <div class="text-display"><span style="background-color: #FFADAD;" title="Chunk 0 | Start: 0 | End: 491 | Tokens: 97"><br>Retrieval-Augmented Generation (RAG) Lewis et al. (2020) has emerged as one of the leading approaches to improving reliability. Using a trusted text corpus to provide factual evidence, RAG guides the LLM&#x27;s output, reducing hallucinations and ensuring closer alignment with the source material Tonmoy et al. (2024). In the context of long, structurally similar legal documents, identifying the relevant text passage as &quot;needle in the haystack&quot; becomes a top priority that we aim to address.<br></span><span style="background-color: #FFD6A5;" title="Chunk 1 | Start: 491 | End: 1425 | Tokens: 169"><br>On a technical level, we quantify the retrieval quality with our Document-Level Retrieval Mismatch (DRM) metric and the character-level precision and recall. Then, we investigate a simple yet effective technique to improve retrieval quality, Summary-Augmented Chunking (SAC). We enrich text chunks in the trusted text corpus with document-level summaries. This preserves global context, lost in standard chunking, guiding the retriever toward the correct document without altering the underlying retrieval pipeline. This method is applied to question-answering tasks across a diverse set of legal documents, including privacy policies, non-disclosure agreements, and merger-and-acquisition contracts.<br><br>Key contributions: (i) First, we define and quantify Document-Level Retrieval Mismatch (DRM), a key failure mode we observe in standard RAG pipelines where the retrieved information originates from entirely wrong source documents. </span><span style="background-color: #FDFFB6;" title="Chunk 2 | Start: 1425 | End: 2784 | Tokens: 251">(ii) We propose Summary-Augmented Chunking (SAC) as a lightweight and modular solution that strongly reduces DRM by injecting global context directly into each chunk. (iii) We demonstrate SAC&#x27;s effectiveness on LegalBench-RAG, a new benchmark comprising question-answering tasks across structurally similar legal documents.<br><br>We measure the performance on LegalBench-RAG via document-level DRM and character-level precision/recall between the retrieved and ground-truth text snippets (see Figure 1a), offering a holistic measure of retrieval quality. While our current work focuses exclusively on this retrieval analysis, we are currently working on adapting a benchmark such as the Australian Legal QA dataset for end-to-end performance evaluation in future work.<br><br>This approach injects crucial global context into each chunk, specifically to mitigate DRM by guiding the retriever to the correct source document. The method is highly practical, requiring only one additional LLM call per document and can be smoothly integrated into existing RAG pipelines with minimal computational overhead.<br><br>The generic prompt used for summarization is the following: &quot;Summarize the main purpose and key obligations of this [document type] in exactly [char_length] characters.&quot; Because LLMs often deviate from the specified length, we allowed a tolerance of 20 characters. </span><span style="background-color: #CAFFBF;" title="Chunk 3 | Start: 2784 | End: 3554 | Tokens: 146">Outputs exceeding this limit were regenerated with a reduced char_length value.<br><br>4 Experimental Setup<br>We evaluate the performance of our methods covering a broad picture of retrieval quality: (i) Document-Level Retrieval Mismatch (DRM): As our primary metric, DRM directly measures the retriever&#x27;s ability to identify the correct source document. A lower DRM indicates higher precision at the document level. (ii) Text-Level Precision: It measures the fraction of all the retrieved text that is part of the ground truth text span. High precision means that the retrieved context is concise and contains minimal irrelevant &quot;noise&quot;. (iii) Text-Level Recall: It measures the fraction of ground truth text captured in the retrieved text. High recall indicates completeness.<br></span><span style="background-color: #9BF6FF;" title="Chunk 4 | Start: 3554 | End: 4037 | Tokens: 96"><br>We experimented with chunk sizes of 200, 500, and 800 characters, combined with prepended summaries of either 150 or 300 characters. The precision and recall results for all six configurations are reported in Table 1. For our final pipeline, we selected a chunk size of 500 characters, consistent with Pipitone and Alami (2024), and a 150-character summary, as this configuration yielded the most balanced trade-off between precision and recall.<br><br>5 Results<br>5.1 Automatic Evaluation<br></span><span style="background-color: #A0C4FF;" title="Chunk 5 | Start: 4037 | End: 4554 | Tokens: 97">We demonstrate that SAC significantly reduces DRM compared to the baseline, showcasing its effectiveness in providing necessary global context. The results, reported in Figure 2b, show a dramatic reduction in DRM across a wide range of hyperparameters, effectively halving the mismatch rate. Crucially, this improvement in document-level accuracy translates directly to improved text-level retrieval quality.<br><br>Across all datasets, SAC improves precision by 12-28% and recall by 8-22%, depending on the document type. </span><span style="background-color: #BDB2FF;" title="Chunk 6 | Start: 4554 | End: 4997 | Tokens: 73">Privacy policies benefited most from SAC due to their repetitive structure, while M&amp;A contracts saw smaller gains from their more unique clause arrangements. Figure 3 shows example retrievals where baseline chunking retrieves irrelevant boilerplate from wrong documents, while SAC consistently pulls from the correct source.<br><br>5.2 Ablation Studies<br>Removing the summary prefix reverts performance to baseline levels, confirming its causal role. </span><span style="background-color: #FFC6FF;" title="Chunk 7 | Start: 4997 | End: 5242 | Tokens: 54">Alternative global context injections (e.g., document titles only) yielded 4-7% DRM reductions, far below SAC&#x27;s 50% average. Increasing summary length beyond 300 characters provided marginal gains (+2% precision) but doubled preprocessing cost.<br></span><span style="background-color: #FFADAD;" title="Chunk 8 | Start: 5242 | End: 5389 | Tokens: 26"><br>6 Discussion<br>Standard fixed-size chunking destroys document-level context, making retrievers vulnerable to DRM in corpora with similar documents. </span><span style="background-color: #FFD6A5;" title="Chunk 9 | Start: 5389 | End: 6643 | Tokens: 223">SAC addresses this without retraining or complex reranking, making it ideal for production RAG systems. Limitations include summary generation cost (one LLM call per document) and potential summary hallucination, though we observed &lt;1% error rate with GPT-4.<br><br>Future work includes end-to-end generation evaluation, multi-document retrieval, and adapting SAC for non-legal domains like scientific papers or financial reports. Integrating SAC with advanced retrievers (e.g., ColBERT) could yield multiplicative gains.<br><br>6 Conclusion<br>We addressed the critical challenge of retrieval reliability in RAG systems operating on large, structurally similar legal document databases. We identified and quantified Document-Level Retrieval Mismatch (DRM) as a dominant failure mode, where retrievers are often easily confused by legal boilerplate language and select text from entirely incorrect documents. Targeting this issue, we investigate Summary-Augmented Chunking (SAC), a simple and computationally efficient technique that prepends document-level summaries to each text chunk. By injecting global context, SAC drastically reduces DRM and consequently improves text-level retrieval precision and recall.<br><br>Appendix A Hyperparameter Chunk Size and Summary Size<br></span><span style="background-color: #FDFFB6;" title="Chunk 10 | Start: 6643 | End: 6750 | Tokens: 31">[Table 1 data would go here showing configs: 200+150, 200+300, 500+150, etc. with precision/recall scores]<br></span><span style="background-color: #CAFFBF;" title="Chunk 11 | Start: 6750 | End: 7114 | Tokens: 79"><br>Appendix B Datasets<br>LegalBench-RAG comprises 420 QA pairs across 3 document types:<br>- Privacy Policies (140 docs, 15k avg chars): GDPR compliance notices<br>- NDAs (120 docs, 8k chars): Standard confidentiality agreements  <br>- M&amp;A Contracts (160 docs, 22k chars): Merger agreements with boilerplate<br><br>All documents sourced from SEC EDGAR filings 2020-2024, anonymized. </span><span style="background-color: #9BF6FF;" title="Chunk 12 | Start: 7114 | End: 7220 | Tokens: 22">Queries generated via GPT-4 with document-specific prompts ensuring diverse span coverage (clauses 2-15).<br></span><span style="background-color: #A0C4FF;" title="Chunk 13 | Start: 7220 | End: 7416 | Tokens: 45"><br>This text mimics real legal docs with repetition, structure, and semantic densityâ€”test semantic chunking vs fixed-size to see how well it preserves &quot;DRM&quot; and &quot;SAC&quot; context across chunks.[web:57]<br></span></div>
</div>

    
<footer>
    Made with <span class="heart">ðŸ¤Ž</span> by <a href="https://github.com/chonkie-inc/chonkie" target="_blank" rel="noopener noreferrer">ðŸ¦› Chonkie</a>
</footer>

</body>
</html>
