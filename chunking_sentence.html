
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chunk Visualization</title>
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAxMDAgMTAwIiB3aWR0aD0iMTAwIiBoZWlnaHQ9IjEwMCI+PHRleHQgeD0iNTAiIHk9IjU1IiBmb250LXNpemU9IjkwIiB0ZXh0LWFuY2hvcj0ibWlkZGxlIiBkb21pbmFudC1iYXNlbGluZT0ibWlkZGxlIj7wn6abPC90ZXh0Pjwvc3ZnPg==">
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol"; line-height: 1.6; padding: 0; margin: 0; background-color: #F0F2F5; color: #333333; display: flex; flex-direction: column; min-height: 100vh; }
        .content-box { max-width: 900px; width: 100%; margin: 30px auto; padding: 30px 20px 20px 20px; background-color: #FFFFFF; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); box-sizing: border-box; }
        .text-display { white-space: pre-wrap; word-wrap: break-word; font-family: "Consolas", "Monaco", "Courier New", monospace; font-size: 0.95em; padding: 0; }
        .text-display span[style*="background-color"] { border-radius: 3px; padding: 0.1em 0; cursor: help; }
        .text-display br { display: block; content: ""; margin-top: 0.6em; }
        footer { text-align: center; margin-top: auto; padding: 15px 0; font-size: 0.8em; color: #888; border-top: 1px solid #eee; background-color: #f0f2f5; width: 100%; }
        footer a { color: #666; text-decoration: none; }
        footer a:hover { text-decoration: underline; }
        footer .heart { color: #d63384; display: inline-block; }
    </style>
</head>
<body>
    
<div class="content-box">
    <div class="text-display"><span style="background-color: #FFADAD;" title="Chunk 0 | Start: 0 | End: 491 | Tokens: 491"><br>Retrieval-Augmented Generation (RAG) Lewis et al. (2020) has emerged as one of the leading approaches to improving reliability. Using a trusted text corpus to provide factual evidence, RAG guides the LLM&#x27;s output, reducing hallucinations and ensuring closer alignment with the source material Tonmoy et al. (2024). In the context of long, structurally similar legal documents, identifying the relevant text passage as &quot;needle in the haystack&quot; becomes a top priority that we aim to address.<br></span><span style="background-color: #FFD6A5;" title="Chunk 1 | Start: 491 | End: 848 | Tokens: 357"><br>On a technical level, we quantify the retrieval quality with our Document-Level Retrieval Mismatch (DRM) metric and the character-level precision and recall. Then, we investigate a simple yet effective technique to improve retrieval quality, Summary-Augmented Chunking (SAC). We enrich text chunks in the trusted text corpus with document-level summaries. </span><span style="background-color: #FDFFB6;" title="Chunk 2 | Start: 848 | End: 1193 | Tokens: 345">This preserves global context, lost in standard chunking, guiding the retriever toward the correct document without altering the underlying retrieval pipeline. This method is applied to question-answering tasks across a diverse set of legal documents, including privacy policies, non-disclosure agreements, and merger-and-acquisition contracts.<br></span><span style="background-color: #CAFFBF;" title="Chunk 3 | Start: 1193 | End: 1592 | Tokens: 399"><br>Key contributions: (i) First, we define and quantify Document-Level Retrieval Mismatch (DRM), a key failure mode we observe in standard RAG pipelines where the retrieved information originates from entirely wrong source documents. (ii) We propose Summary-Augmented Chunking (SAC) as a lightweight and modular solution that strongly reduces DRM by injecting global context directly into each chunk. </span><span style="background-color: #9BF6FF;" title="Chunk 4 | Start: 1592 | End: 1975 | Tokens: 383">(iii) We demonstrate SAC&#x27;s effectiveness on LegalBench-RAG, a new benchmark comprising question-answering tasks across structurally similar legal documents.<br><br>We measure the performance on LegalBench-RAG via document-level DRM and character-level precision/recall between the retrieved and ground-truth text snippets (see Figure 1a), offering a holistic measure of retrieval quality. </span><span style="background-color: #A0C4FF;" title="Chunk 5 | Start: 1975 | End: 2338 | Tokens: 363">While our current work focuses exclusively on this retrieval analysis, we are currently working on adapting a benchmark such as the Australian Legal QA dataset for end-to-end performance evaluation in future work.<br><br>This approach injects crucial global context into each chunk, specifically to mitigate DRM by guiding the retriever to the correct source document. </span><span style="background-color: #BDB2FF;" title="Chunk 6 | Start: 2338 | End: 2784 | Tokens: 446">The method is highly practical, requiring only one additional LLM call per document and can be smoothly integrated into existing RAG pipelines with minimal computational overhead.<br><br>The generic prompt used for summarization is the following: &quot;Summarize the main purpose and key obligations of this [document type] in exactly [char_length] characters.&quot; Because LLMs often deviate from the specified length, we allowed a tolerance of 20 characters. </span><span style="background-color: #FFC6FF;" title="Chunk 7 | Start: 2784 | End: 3193 | Tokens: 409">Outputs exceeding this limit were regenerated with a reduced char_length value.<br><br>4 Experimental Setup<br>We evaluate the performance of our methods covering a broad picture of retrieval quality: (i) Document-Level Retrieval Mismatch (DRM): As our primary metric, DRM directly measures the retriever&#x27;s ability to identify the correct source document. A lower DRM indicates higher precision at the document level. </span><span style="background-color: #FFADAD;" title="Chunk 8 | Start: 3193 | End: 3688 | Tokens: 495">(ii) Text-Level Precision: It measures the fraction of all the retrieved text that is part of the ground truth text span. High precision means that the retrieved context is concise and contains minimal irrelevant &quot;noise&quot;. (iii) Text-Level Recall: It measures the fraction of ground truth text captured in the retrieved text. High recall indicates completeness.<br><br>We experimented with chunk sizes of 200, 500, and 800 characters, combined with prepended summaries of either 150 or 300 characters. </span><span style="background-color: #FFD6A5;" title="Chunk 9 | Start: 3688 | End: 4181 | Tokens: 493">The precision and recall results for all six configurations are reported in Table 1. For our final pipeline, we selected a chunk size of 500 characters, consistent with Pipitone and Alami (2024), and a 150-character summary, as this configuration yielded the most balanced trade-off between precision and recall.<br><br>5 Results<br>5.1 Automatic Evaluation<br>We demonstrate that SAC significantly reduces DRM compared to the baseline, showcasing its effectiveness in providing necessary global context. </span><span style="background-color: #FDFFB6;" title="Chunk 10 | Start: 4181 | End: 4554 | Tokens: 373">The results, reported in Figure 2b, show a dramatic reduction in DRM across a wide range of hyperparameters, effectively halving the mismatch rate. Crucially, this improvement in document-level accuracy translates directly to improved text-level retrieval quality.<br><br>Across all datasets, SAC improves precision by 12-28% and recall by 8-22%, depending on the document type. </span><span style="background-color: #CAFFBF;" title="Chunk 11 | Start: 4554 | End: 4997 | Tokens: 443">Privacy policies benefited most from SAC due to their repetitive structure, while M&amp;A contracts saw smaller gains from their more unique clause arrangements. Figure 3 shows example retrievals where baseline chunking retrieves irrelevant boilerplate from wrong documents, while SAC consistently pulls from the correct source.<br><br>5.2 Ablation Studies<br>Removing the summary prefix reverts performance to baseline levels, confirming its causal role. </span><span style="background-color: #9BF6FF;" title="Chunk 12 | Start: 4997 | End: 5493 | Tokens: 496">Alternative global context injections (e.g., document titles only) yielded 4-7% DRM reductions, far below SAC&#x27;s 50% average. Increasing summary length beyond 300 characters provided marginal gains (+2% precision) but doubled preprocessing cost.<br><br>6 Discussion<br>Standard fixed-size chunking destroys document-level context, making retrievers vulnerable to DRM in corpora with similar documents. SAC addresses this without retraining or complex reranking, making it ideal for production RAG systems. </span><span style="background-color: #A0C4FF;" title="Chunk 13 | Start: 5493 | End: 5919 | Tokens: 426">Limitations include summary generation cost (one LLM call per document) and potential summary hallucination, though we observed &lt;1% error rate with GPT-4.<br><br>Future work includes end-to-end generation evaluation, multi-document retrieval, and adapting SAC for non-legal domains like scientific papers or financial reports. Integrating SAC with advanced retrievers (e.g., ColBERT) could yield multiplicative gains.<br></span><span style="background-color: #687fa5;" title="Chunk 13 | Start: 5493 | End: 5919 | Tokens: 426 (Overlap)"><br>6 Conclusion<br></span><span style="background-color: #BDB2FF;" title="Chunk 14 | Start: 5905 | End: 6283 | Tokens: 378">We addressed the critical challenge of retrieval reliability in RAG systems operating on large, structurally similar legal document databases. We identified and quantified Document-Level Retrieval Mismatch (DRM) as a dominant failure mode, where retrievers are often easily confused by legal boilerplate language and select text from entirely incorrect documents. </span><span style="background-color: #FFC6FF;" title="Chunk 15 | Start: 6283 | End: 6771 | Tokens: 488">Targeting this issue, we investigate Summary-Augmented Chunking (SAC), a simple and computationally efficient technique that prepends document-level summaries to each text chunk. By injecting global context, SAC drastically reduces DRM and consequently improves text-level retrieval precision and recall.<br><br>Appendix A Hyperparameter Chunk Size and Summary Size<br>[Table 1 data would go here showing configs: 200+150, 200+300, 500+150, etc. with precision/recall scores]<br></span><span style="background-color: #a580a5;" title="Chunk 15 | Start: 6283 | End: 6771 | Tokens: 488 (Overlap)"><br>Appendix B Datasets<br></span><span style="background-color: #FFADAD;" title="Chunk 16 | Start: 6750 | End: 7220 | Tokens: 470">LegalBench-RAG comprises 420 QA pairs across 3 document types:<br>- Privacy Policies (140 docs, 15k avg chars): GDPR compliance notices<br>- NDAs (120 docs, 8k chars): Standard confidentiality agreements  <br>- M&amp;A Contracts (160 docs, 22k chars): Merger agreements with boilerplate<br><br>All documents sourced from SEC EDGAR filings 2020-2024, anonymized. Queries generated via GPT-4 with document-specific prompts ensuring diverse span coverage (clauses 2-15).<br></span><span style="background-color: #FFD6A5;" title="Chunk 17 | Start: 7220 | End: 7416 | Tokens: 196"><br>This text mimics real legal docs with repetition, structure, and semantic densityâ€”test semantic chunking vs fixed-size to see how well it preserves &quot;DRM&quot; and &quot;SAC&quot; context across chunks.[web:57]<br></span></div>
</div>

    
<footer>
    Made with <span class="heart">ðŸ¤Ž</span> by <a href="https://github.com/chonkie-inc/chonkie" target="_blank" rel="noopener noreferrer">ðŸ¦› Chonkie</a>
</footer>

</body>
</html>
